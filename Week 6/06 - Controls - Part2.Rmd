---
title: "FEM11213 - Data Science and HR Analytics"
subtitle: "06 - Controls"
author: "Sacha Kapoor"
date: "03 November 2023 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(eval = TRUE,
               echo = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE)

htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r plots_and_figures, include=FALSE}
library(ggplot2)
library(kableExtra)

theme_set(theme_gray(15))
```

```{r, references, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "numeric", 
           cite.style = "authoryear", 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
bib <- ReadBib("../refs/references.bib", check = FALSE)
```


class: title-slide-section-gray


# Orthogonal ML for LTE

.pull-left[

ALGORITHM 15 (Orthogonal ML for LTE):

Split data randomly into $K$ folds of roughly the same size. 

  i. *Nuisance estimation*: For $k = 1,\ldots, K$, 
  
  * Use ML tools to fit prediction functions
    
    $\hat{\mathbb{E}}_{-k}[d|\mathbf{x}]$ and $\hat{\mathbb{E}}_{-k}[y|\mathbf{x}]$
    
  on all folds other than $k$. 
    
  * Calculate out of sample residuals on the $k$th fold: 
  
$\tilde{d}_i = d_i -  \hat{\mathbb{E}}_{-k}[d|\mathbf{x}]$ and  $\tilde{y}_i = y_i -  \hat{\mathbb{E}}_{-k}[d|\mathbf{x}]$.   
  
  ii. *Treatment effect inference*: 
  
  * Collect OOS residuals from nuisance stage 
    
  * Estimate $\mathbb{E}[\tilde{y}|\tilde{d}] = \alpha + \gamma \tilde{d}$. 
]

.pull-right[

* Algorithm is more general than simple sample splitting: 
  + don't throw away part of your data 

* Algorithm looks like cross validation. 
  + But objective is different here. 
    - Objective is estimation here. 
    - Before the objective was prediction. 

]

---

```{r, echo=FALSE}
library(Matrix)
data <- read.table("../Data/abortion.dat", skip=1, sep="\t")
names(data) <- c("state","year","pop","y_viol","y_prop","y_murd",
	"a_murd","a_viol","a_prop",'prison','police',
	'ur','inc','pov','afdc','gun','beer')
data <- data[!(data$state%in%c(2,9,12)),] # AK, DC, HA are strange places
data <- data[data$year>84 & data$year<98,] # incomplete data outside these years
data$pop <- log(data$pop)
t <- data$year - 85
s <- factor(data$state) ## states are numbered alphabetically
controls <- data.frame(data[,c(3,10:17)])
## y is de-trended log crime rate, a is as described below
## note we also have violent and property crime versions
y <- data$y_murd
d <- data$a_murd
cell <- read.csv("../Data/us_cellphone.csv")
cellrate <- 5*cell[,2]/(1000*cell[,3]) # center on 1985 and scale by 1997-1985
phone <- cellrate[ t + 1 ]
t <- factor(t)
sna <- factor(s, levels=c(NA,levels(s)), exclude=NULL)
x <- sparse.model.matrix( ~ t + phone*sna + .^2, data=controls)[,-1]
```

```{r}
library(AER)
library(gamlr)

dreg <- function(x,d){ cv.gamlr(x, d, lmr=1e-5) }
yreg <- function(x,y){ cv.gamlr(x, y, lmr=1e-5) }
```

---

```{r}
# Orthogonal ML R Function

orthoLTE <- function(x, d, y, dreg, yreg, nfold=2)
{
	# randomly split data into folds
	nobs <- nrow(x)
    foldid <- rep.int(1:nfold, 
    	times = ceiling(nobs/nfold))[sample.int(nobs)]
    I <- split(1:nobs, foldid)
    # create residualized objects to fill
	ytil <- dtil <- rep(NA, nobs)
	# run OOS orthogonalizations
	cat("fold: ")
	for(b in 1:length(I)){
		dfit <- dreg(x[-I[[b]],], d[-I[[b]]])
		yfit <- yreg(x[-I[[b]],], y[-I[[b]]])
		dhat <- predict(dfit, x[I[[b]],], type="response")
		yhat <- predict(yfit, x[I[[b]],], type="response")
		dtil[I[[b]]] <- drop(d[I[[b]]] - dhat)
		ytil[I[[b]]] <- drop(y[I[[b]]] - yhat)
		cat(b," ")
	}
	rfit <- lm(ytil ~ dtil)
	gam <- coef(rfit)[2]
	se <- sqrt(vcovHC(rfit)[2,2])
	cat(sprintf("\ngamma (se) = %g (%g)\n", gam, se))

	return( list(gam=gam, se=se, dtil=dtil, ytil=ytil) )
}
```

---
```{r}
# OrthoML and effect of abortion access on crime

resids <- orthoLTE( x=x, d=d, y=y, 
				dreg=dreg, yreg=yreg, nfold=5) 
head(resids$dtil)
head(resids$ytil)
2*pnorm(-abs(resids$gam)/resids$se) #p-value supports no effect of abortion access on crime
```

---

# Heterogeneous Treatment Effects (HTE)

Last method we cover in this course. 

Our baseline econometric model of interest was 

\begin{align*}
y = \beta_0 + \beta_1 d + \mathbf{x}'\boldsymbol\beta + e\qquad 
\end{align*}

where 

\begin{align*}
\mathbf{x}'\boldsymbol\beta = \beta_2 x_2 + \cdots + \beta_p x_p 
\end{align*}


Baseline model assumes treatment effect is same for everyone. Not reasonable. More likely that $\beta_1$ is specific to the individual or groups of individuals. 

One way to capture this heterogeneity is to redefine econometric model such that: 

\begin{align*}
y = \beta_0 + \beta_1(\mathbf{x}) d + \mathbf{x}'\boldsymbol\beta + e\qquad 
\end{align*}

where $\beta_1(\mathbf{x}) = \beta_1 + \mathbf{x}'\gamma$ is the treatment effect for all people with a given $\mathbf{x}$.  

---

# Heterogeneous Treatment Effects (HTE)

To study HTE, we look at data generated by the Oregon Health Insurance Experiment (OHIE). Specific interest is in effect of randomly becoming eligible for health insurance on propensity to vist a primary care physician (PCP). 

```{r, echo=FALSE}
# person_id  is key
# treatment is in Description file, and is random conditional on the numhh_list (number of names in lottery)
# in 2008 new spots opened for medicaid, which was previously closed to new enroll
# we are interested in health insurance effect on increased costs and utilization (on health is longer term)
# admin data is clean, survey data no necessarily balanced due to non-response bias
# admin data has hospital admission (by dept, emerg itself is non-signif)
# we can also look at number of hostpital days or total list cost

library(foreign)

descr <- read.dta("../Data/oregonhie_descriptive_vars.dta")
prgm <- read.dta("../Data/oregonhie_stateprograms_vars.dta")
s12 <- read.dta("../Data/oregonhie_survey12m_vars.dta")

# nicely organized, one row per person
all(s12$person_id == descr$person_id)
all(s12$person_id == prgm$person_id)

P <- descr[,c("person_id","household_id", "numhh_list")]
P$medicaid <- as.numeric(prgm[,"ohp_all_ever_firstn_30sep2009"]=="Enrolled")
P$selected <- as.numeric(descr[,"treatment"]=="Selected")
levels(P$numhh_list) <- c("1","2","3+")

# 12 month is the survey that really matters
# need to control for household size interacted with survey return time
Y <- s12[,c("weight_12m",
	"doc_any_12m","doc_num_mod_12m",
	"er_any_12m","er_num_mod_12m",
	"hosp_any_12m","hosp_num_mod_12m")]
Y$doc_any_12m <- as.numeric(Y$doc_any_12m=="Yes")
Y$er_any_12m <- as.numeric(Y$er_any_12m=="Yes")
Y$hosp_any_12m <- as.numeric(Y$hosp_any_12m=="Yes")

# smk_ever_12m - num19_12m are sources of heterogeneity, plus descr
X <- s12[,121:147]
X$dt_returned <- factor(format(s12$dt_returned_12m, "%Y-%m"))

insurv <- which(s12$sample_12m_resp == "12m mail survey responder")
X <- X[insurv,]
Y <- Y[insurv,]
P <- P[insurv,]

sapply(Y,function(y) sum(is.na(y)))
nomiss <- which( !apply(Y,1, function(y) any(is.na(y))) )
X <- X[nomiss,]
Y <- Y[nomiss,]
P <- P[nomiss,]

# pull out the weights and attach doc_any to P
weights <- Y[,1]
Y <- Y[,-1]

# replace some ridiculous values in survey and drop num19
X$hhsize_12m[X$hhsize_12m>10] <- 10
X$num19_12m <- NULL

# organize to make it pretty for text
P$doc_any_12m <- Y$doc_any_12m # you can explore other responses if you want
P <- P[,c(1,2,6,5,4,3)]
names(P)[6] <- "numhh"
```

```{r}
# data has been cleaned in the background
head(P,n=3)
dim(P)
table(P$selected)
```

---

Average effects can be computed as follows 

```{r}
ybar <- tapply(P$doc_any_12m, P$selected, mean)
( ATE = ybar['1'] - ybar['0'] )

nsel <- table(P[,c("selected")])
yvar <- tapply(P$doc_any_12m, P$selected, var)
( seATE = sqrt(sum(yvar/nsel)) )

ATE + c(-2,2)*seATE
```

--- 

Control for number of household members because randomization was imperfect. Randomized across households. If your household was chosen, then everyone in your household became eligible. 

```{r}
lin <- glm(doc_any_12m ~ selected + numhh, data=P);
round( summary(lin)$coef["selected",],4) # 6-7% increase in prob
```


---

Objective is to use ML tools to study heterogeneous treatment effects in this experiment. 

Want to leave coefficient on numhh unpenalized because it encapsulates imperfectness of randomization. 

But we have a missing data problem. Let's deal with that first. 


---

# Digression - Handling Missings 

  * Categorical variables: 
    + introduce a new category called "NA" 
    ```{r}
    levels(X$edu_12m)
    source("naref.R")
    levels(naref(X$edu_12m))
    X <- naref(X) #makes NA the base group
    ```
  * Continuous variables: 
    + set the missing values to 0 or the sample mean

---

```{r}
xnum <- X[,sapply(X,class)%in%c("numeric","integer")]
xnum[66:70,]
colSums(is.na(xnum))
# flag missing
xnumna <- apply(is.na(xnum), 2, as.numeric)
xnumna[66:70,]
```

---

```{r}
# impute the missing values
mzimpute <- function(v){ 
	if(mean(v==0,na.rm=TRUE) > 0.5) impt <- 0
	else impt <- mean(v, na.rm=TRUE)
	v[is.na(v)] <- impt
	return(v) }
xnum <- apply(xnum, 2,  mzimpute)
xnum[66:70,]
```

---

```{r}
# replace/add the variables in new data frame 
for(v in colnames(xnum)){
	X[,v] <- xnum[,v]
	X[,paste(v,"NA", sep=".")] <- xnumna[,v] }
X[144:147,]
```

---

After filling in the missings, put everything into a sparse model matrix. Add numhh to control for imperfect randomization.

```{r}
xhte <- sparse.model.matrix(~., data=cbind(numhh=P$numhh, X))[,-1]
xhte[1:2,1:4]
dim(xhte)
```

Now we are ready to deal with heterogeneous treatment effects. 

---

```{r}
dxhte <- P$selected*xhte
colnames(dxhte) <- paste("d",colnames(xhte), sep=".")
htedesign <- cbind(xhte,d=P$selected,dxhte)
# include the numhh controls and baseline treatment without penalty 
htefit <- gamlr(x=htedesign, y=P$doc_any_12m, free=c("numhh2","numhh3+","d"))
gam <- coef(htefit)[-(1:(ncol(xhte)+1)), ]
round(sort(gam)[1:6],4)
round(sort(gam, decreasing=TRUE)[1:6],4)
```

Treatment effect is now a 9% increase in PCP visit rates. 
  * But significant heterogeneity around this value. 
  * e.g. treatment effect is 13% for people of Pacific Islander descent.

---

# Consumer demand estimation and heterogeneous treatment effects 

Price elasticity of demand is a key parameter for many businesses. Measures the percentage change in quantity demand for a percentage change in price. 

  $\gamma = \frac{\Delta q/q}{\Delta p/p}$

It is key for many businesses because it determines the gross margin: 

  $p = \frac{\gamma}{1 + \gamma} c$

It is important for them to know how it depends on consumer characteristics like age (for price discrimination e.g.). To estimate heterogeneous price elasticities, can specify: 

\begin{align*}
ln(q_{it}) &= \gamma ln(p_{it}) + \mathbf{x}_{it}'\boldsymbol\beta + \varepsilon_{it}\\
ln(p_{it}) &= \mathbf{x}_{it}'\boldsymbol\tau + v_{it}\\
\end{align*}
where $i$ is product, $t$ is the week, $\mathbb{E}[\varepsilon_{it}|p_{it}, \mathbf{x}_{it}'] = 0$ and $\mathbb{E}[v_{it}|\mathbf{x}_{it}'] = 0$. Taddy interprets $\mathbf{x}_{it}$ as demand signals.  

---

```{r}
load("../Data/dominicks-beer.rda")
head(wber)
wber = wber[sample(nrow(wber), 100000), ]
head(upc)
dim(upc)
wber$lp <- log(12*wber$PRICE/upc[wber$UPC,"OZ"]) #ln price per 12 ounces
```

---

```{r}
coef( margfit <- lm(log(MOVE) ~ lp, data=wber[,]) )
#10% increase in price decreases  quantity sold by 6%
#ATE
```

---

```{r}
wber$s <- factor(wber$STORE); wber$u <- factor(wber$UPC); wber$w <- factor(wber$WEEK)
xs <- sparse.model.matrix( ~ s-1, data=wber); xu <- sparse.model.matrix( ~ u-1, data=wber); xw <- sparse.model.matrix( ~ w-1, data=wber)
# parse the item description text as a bag o' words
library(tm)
descr <- Corpus(VectorSource(as.character(upc$DESCRIP)))
descr <- DocumentTermMatrix(descr)
descr <- sparseMatrix(i=descr$i,j=descr$j,x=as.numeric(descr$v>0), # convert from stm to Matrix format
              dims=dim(descr),dimnames=list(rownames(upc),colnames(descr)))
descr[1:5,1:6]
descr[287,descr[287,]!=0]
controls <- cbind(xs, xu, xw, descr[wber$UPC,]) 
dim(controls)

```


---

```{r}
# naive lasso
naivefit <- gamlr(x=cbind(lp=wber$lp,controls)[,], y=log(wber$MOVE), free=1, standardize=FALSE)
print( coef(naivefit)[1:2,] )
# orthogonal ML 
resids <- orthoLTE( x=controls, d=wber$lp, y=log(wber$MOVE), dreg=dreg, yreg=yreg, nfold=5)
```
---


```{r}
# interact items and text with price
#lpxu <- xu*wber$lp
#colnames(lpxu) <- paste("lp",colnames(lpxu),sep="")
# create our interaction matrix
xhte <- cbind(BASELINE=1,descr[wber$UPC,])
d <- xhte*wber$lp
colnames(d) <- paste("lp",colnames(d),sep=":")

eachbeer <- xhte[match(rownames(upc),wber$UPC),]
rownames(eachbeer) <- rownames(upc)
# fullhte
lnwberMOVE <- log(wber[['MOVE']])
fullhte <- gamlr(x=cbind(d,controls), y=lnwberMOVE, lambda.start=0)
#gamfull <- coef(fullhte)[2:(ncol(lpxu)+1),]
gamfull <- drop(eachbeer%*%coef(fullhte)[2:(ncol(d)+1),])
```


---
```{r}
coef(fullhte)
```

---

```{r}

hist(gamfull, main="", xlab="elasticity", col="darkgrey", freq=FALSE)
```

---

# References

```{r, 'refs1', results='asis', echo=FALSE, eval=TRUE}
PrintBibliography(bib, start = 1, end = 6)
```
