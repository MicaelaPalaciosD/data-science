---
title: "FEM11213 - Data Science and HR Analytics"
subtitle: "06 - Controls"
author: "Sacha Kapoor changed by Micaela Palacios"
date: "03 November 2023 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(eval = TRUE,
               echo = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE)

htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r plots_and_figures, include=FALSE}
library(ggplot2)
library(kableExtra)

theme_set(theme_gray(15))
```

```{r, references, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "numeric", 
           cite.style = "authoryear", 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
bib <- ReadBib("../refs/references.bib", check = FALSE)
```


class: title-slide-section-gray

# Controls 

Learning objectives in this section: 

  * how to do counterfactual analysis using conditional independence (CI), aka conditional ignorability?
  
  * how do ML tools help facilitate conditional independence
  
  * how do ML tools help facilitate understanding of heterogeneous treatment effects
  
Our discussion draws heavily on Chapter 6 of `r Citet(bib, "Taddy19")`. 

---

# Conditional Independence and Linear Treatment Effects 

Causal identification is a key goal in applied econometrics. Idea is as follows. Suppose you observe everyone in your target population. You want to know if you can recover the true effect of $d$ on $y$ 

\begin{align*}
y = \beta_0 + \beta_1 d + e.
\end{align*}

where "true" effect is $\beta_1$. True effect cannot usually be recovered because $e$ reflects ALL omitted variables $x_2, x_3, \ldots, x_p$ which covary with both $d$ and $y$. Identification can be restored if can bring these omitted variables into our model:

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e.
\end{align*}

This (identification) would be the case if 

\begin{align*}
\mathbb{E}[e|d, x_2, \ldots, x_p]=0.
\end{align*}

Process of choosing controls $x_2, x_3, \ldots, x_p$ can be highly subjective and labor intensive. Process becomes trickier when $x_2, x_3, \ldots, x_p$ is high dimensional or complex functional forms dictate influence of $x_2, x_3, \ldots, x_p$ on $d$ and $y$. 

ML provides a means of choosing controls that is less subjective, less labor intensive, and that works well with high dimensionality and or unknown functional forms.  

---

# Example - Estimating consumer demand elasticities 

log-log regression is used to estimate elasticities: 

```{r}
oj <- read.csv("../Data/oj.csv")
basefit <- lm(log(sales) ~ log(price), data=oj)
coef(basefit)
```
Estimate implies 10 percent increase in price decreases sales by 16 percent. Can we interpret this estimate as causal? 
NO

---

# Example - Estimating consumer demand elasticities 

log-log regression is used to estimate elasticities: 

```{r}
oj <- read.csv("../Data/oj.csv")
basefit <- lm(log(sales) ~ log(price), data=oj)
coef(basefit)
```
Estimate implies 10 percent increase in price decreases sales by 16 percent. Can we interpret this estimate as causal? One factor that jointly influences price and sales is brand name

```{r}
brandfit <- lm(log(sales) ~ brand + log(price), data=oj)
coef(brandfit)
```

Partial effect estimate implies 10 percent increase in price decreases sales by 31 percent. --> partial effect cause we fix the brand and use changes to recover the effects. 

Including the brand gives us a different number

---

# Example - Estimating consumer demand elasticities 

Why did I refer to the second estimate as an estimate of a partial effect? 

---

# Example - Estimating consumer demand elasticities 

Why did I refer to the second estimate as an estimate of a partial effect? Because we are partialling out the variation in price that is explained by brand: 

```{r}
pricereg <- lm(log(price) ~ brand, data=oj) #ln regression
phat <- predict(pricereg, newdata=oj) #p hat from ^pi0 + ^pi1 Brand (prediction)
presid <- log(oj$price) - phat #ln(p)-^ln(p) difference with prediction
residfit <- lm(log(sales) ~ presid, data=oj) #ln(p)=B0* +BIpresid+u
coef(basefit)
```

Taddy has a nice explanation for what OLS is doing: 

  * Part of price can be explained by brand. 
  * The part that cannot be explained by brand is independent of brand.
  * OLS finds the effect of the part of price that is independent of brand. 

---

# Linear Treatment Effects Model

The last example can be modelled using the linear treatment effects (LTE) model: 

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e\qquad \mathbb{E}[e|d,x_2,\ldots,x_p] = 0\\
d = \tau_0 + \tau_2 x_2 + \cdots + \tau_p x_p + v\qquad \mathbb{E}[v|x_2,\ldots,x_p] = 0 
\end{align*}

This model shows formally that OLS is estimating the effect of the residual $v$ on $y$. What is the value added of this second equation? 

If I think that the covariate is important for estimating d, Im implicitly saying they are related.

d is also determined by v, v is unrelated to the xs--> coin flip --> randomized control trial

---

# Linear Treatment Effects Model

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e\qquad \mathbb{E}[e|d,x_2,\ldots,x_p] = 0\\
d = \tau_0 + \tau_2 x_2 + \cdots + \tau_p x_p + v\qquad \mathbb{E}[v|x_2,\ldots,x_p] = 0 
\end{align*}

Assumption 2: all covariates explain all variation in $d$. Anything left in $v$ is not useful for understanding d and its relationship with $y$ (perfect world)
Second equation adds value because it facilitates interpretation. Second equation forces you to think about the treatment assignment procedure. This procedure explains why some units are assigned to treatment while others are assigned to control. 

Examples: 

  * *Coin flip* determines whether worker is paid a bonus or not. $y$ is worker output.  
  
  * Treatment assignment procedure is easy. 
    + Worker assigned to bonus treatment $d=1$ if heads, 
    + Worker assigned to control $d=0$ if tails. 
    
  * No controls are needed. 

---

# Linear Treatment Effects Model

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e\qquad \mathbb{E}[e|d,x_2,\ldots,x_p] = 0\\
d = \tau_0 + \tau_2 x_2 + \cdots + \tau_p x_p + v\qquad \mathbb{E}[v|x_2,\ldots,x_p] = 0 
\end{align*}

Second equation adds value because it facilitates interpretation. Second equation forces you to think about the treatment assignment procedure. This procedure explains why some units are assigned to treatment while others are assigned to control. 

Examples: 

  * Worker can select into jobs that pay bonuses or not. 
  
  * Treatment assignment procedure is complex: 
    + Worker assigned to bonus treatment $d=1$ if their preferences and productivity make these jobs worthwhile, 
    + Worker assigned to control $d=0$ otherwise. 
  
  * Controls are needed. Want to control for things that influence worker output as well as propensity to work in jobs that pay bonuses (risk aversion, confidence, etc.). 


---

# Linear Treatment Effects Model

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e\qquad \mathbb{E}[e|d,x_2,\ldots,x_p] = 0\\
d = \tau_0 + \tau_2 x_2 + \cdots + \tau_p x_p + v\qquad \mathbb{E}[v|x_2,\ldots,x_p] = 0 
\end{align*}

Second equation can have additional practical value when data are high dimensional $n<p$ and when you need to be selective with controls. Dimensionality means don't have enough data $n$ to estimate first equation. 

  * cannot include all control variables in first equation. Need to be selective.

  * Process of selecting controls becomes too labor intensive and subjective. 
  
  * Can apply ML to second equation to reduce the set of control variables (to be less than $n$). Especially valuable when you have more controls than data points. 


---

# Example

We illustrate the application of ML within the context of an example. In the example, the causal question of interest is: does access to abortion decrease crime? Accordingly, we estimate 

\begin{align*}
y_{st} = \alpha_s + \beta_1 d_{st} + \beta_2 x_{2st} + \cdots + \beta_p x_{pst} + \gamma_t + e_{st} 
\end{align*}
.pull-left[
where 

  * $s$ indexes states in the US, $t$ indexes years  
  * $y_{st}$ is murder rate 
  * $d_{st}$ is effective abortion rate (number of legal abortions per 10 live births.)
  * $\alpha_s$ is a fixed effect for the state 
  * $\gamma_t$ is a linear time trend 
  * Controls include: 
    + prison: Log number of prisoners per capita
    + police: Log number of police per capita
    + ur: Current unemployment rate
    + inc: Current per capita income
    + pov: Current poverty rate
    + AFDC: A measure of charitable generosity at year t âˆ’ 15
    + gun: A dummy for existence of a concealed weapons law
    + beer: The current beer consumption per capita
]
.pull-right[

time effect evolves linearly, if you put dummmies you allow time effect to jump in any direction so you allow time evolving trends to move in a specific way

Time effects linearly is less flexible. linear trend assumes that $x$ moves $y$ in same direction
FE Risk of overfitting is small and better cause allows flexibility of $x$s moving anywhere

---

# Example 

```{r, echo=FALSE}
data <- read.table("../Data/abortion.dat", skip=1, sep="\t")
names(data) <- c("state","year","pop","y_viol","y_prop","y_murd",
	"a_murd","a_viol","a_prop",'prison','police',
	'ur','inc','pov','afdc','gun','beer')
data <- data[!(data$state%in%c(2,9,12)),] # AK, DC, HA are strange places
data <- data[data$year>84 & data$year<98,] # incomplete data outside these years so we limit to above 84 and under 98
data$pop <- log(data$pop) #take the log
t <- data$year - 85 
s <- factor(data$state) ## states are numbered alphabetically
controls <- data.frame(data[,c(3,10:17)])
## y is de-trended log crime rate, a is as described below
## note we also have violent and property crime versions
y <- data$y_murd
d <- data$a_murd
```

```{r}
#we do the regression and show summary
summary(orig <- glm(y ~ d + t + s +., data=controls) )$coef['d',]

#convert it to interpretable numbers
dcoef <- summary(orig <- glm(y ~ d + t + s +., data=controls) )$coef['d',][1]

exp(dcoef) - 1
```

One more abortion per ten live births (the units of d) decreases the per capita murder rate by 19%. Estimate is statistically significant at the 1 percent level. Reject null hypothesis of no effect okf abortion rate on per capita murder rate. 

---

.pull-left[

Taddy looks for a variable that simultaneously tracks abortion rates and correlates with murder rates. He argues that cell phone usage is such variable: 

```{r}
cell <- read.csv("../Data/us_cellphone.csv")
cellrate <- 5*cell[,2]/(1000*cell[,3]) # center on 1985 and scale by 1997-1985
```
Both move in cuadratic fashion, which shows that llinear trend does not make sense.
]

.pull-right[

```{r}
par(mai=c(.9,.9,.1,.1))
plot(1985:1997, tapply(d, t, mean), bty="n", xlab="year", ylab="rate", pch=21, bg=2)
points(1985:1997, cellrate, bg=4, pch=21)
legend("topleft", fill=c(2,4), legend=c("abortions","cellphones"), bty="n")
```  
]

---

# Example 

```{r}
phone <- cellrate[ t + 1 ]
tech <- summary(glm(y ~ phone + t + s +., data=controls))$coef['phone',]
phonecoef <- tech[1]
exp(phonecoef) - 1

```

One more cell phone per every 5 Americans abortion per ten live births (the units of d) decreases the per capita murder rate by 31%. Magnitude is similar to the estimated abortion effect. 

Instead of using abortions, taddy used cellphone data and showed that it decreases murder rate by 31% order of magnitude is comparable. Both are big.

---

# Example 

Problem here is that murder, abortion, cell phone subscription rates are all increasing at a quadratic rate over time. Our baseline regression assumes they only increase linearly with time. Can adapt the baseline regression to include 

  1. fixed effects for the year
    + allows for quadratic time variation
    + but also more flexible forms of time variation 

  2. interactions between the control variables 
    + e.g. a a different gun effect for high beer

  3. phone plus interactions of phone with the state

```{r}
t <- factor(t)
interact <- glm(y ~ d + t + phone*s + .^2, data=controls) #phone interactive with state and we also include quuadratic
summary(interact)$coef["d",]
```

Sign changes. No statistical significance. 

---

# Example 
.pull-left[
  * Sign and significance changes because asking too much of data. 
    + Last specification had $p\approx n$ controls. 
    + Data overfit. 

  * Does this mean initial estimate/conclusion is wrong? No. 
  
  * Point is instead that if conditional independence underlies causal interpretation, then researchers can "overturn" your result simply by overfitting the data. 

  * Weakness of OLS as a regression tool - better suited for low dimensional (low $p$) data. 
]
.pull-right[
* overfitting --> adding as many covariates kills efect.
* There is a balance with adjusting with things related to quadratic nature and overfitting.
* With OLS you can laways just add covariates and kill the relation with the data
---

# Back to LTE Model 

Recall (not OLS anymore now Lasso and CV)

\begin{align*}
y = \beta_0 + \beta_1 d + \beta_2 x_2 + \cdots + \beta_p x_p + e\qquad \mathbb{E}[e|d,x_2,\ldots,x_p] = 0\\
d = \tau_0 + \tau_2 x_2 + \cdots + \tau_p x_p + v\qquad \mathbb{E}[v|x_2,\ldots,x_p] = 0 
\end{align*}

Can use use ML methods (LASSO and cross validation) to explore robustness in higher dimensional data. 

Key distinction from standard approach: estimate and combine first and second stage equations. 

---

# Back to LTE Model 

.pull-left[

ALGORITHM 14 (LTE Lasso Regression):

  i. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[d|\mathbf{x}]=\mathbf{x}'\tau$. 
  
  Collect fitted values $\hat{d}= \mathbf{x}'\hat{\tau}$.
  
  ii. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[y|\mathbf{x},d] = \phi\hat{d} + \beta_1d + \mathbf{x}'\beta$
  
  Do not penalize $\phi$. 
]

.pull-right[

Theory behind algorithm. Second stage model is 

$\mathbb{E}[y|\mathbf{x},d]= \beta_1 d + \mathbf{x}'\beta$

From first stage have

$\mathbb{E}[y|\mathbf{x},d]= \beta_1 (\mathbf{x}'\tau + v) + \mathbf{x}'\beta =  \beta_1 v + \mathbf{x}'\beta^*$
(if we plug in d we get b*, combination of tao and beta)

where $\beta^* = (\beta_1\tau + \beta)$. Causal effect $\beta_1$ can be obtained by a regression of $y$ on error $v$. 

In practice, can substitute residual $\hat{v} = d - \hat{d}$ for $v$ ($v$ is not related to $d$ which is why it allows us to recover b):

$\mathbb{E}[y|\mathbf{x},d]= \beta_1 (d - \hat{d}) + \mathbf{x}'\beta^*$ or, equivalently, 

$\mathbb{E}[y|\mathbf{x},d]= \phi\hat{d} + \beta_1 d + \mathbf{x}'\beta^*$

where $\phi = -\beta_1$. Do not penalize $\phi$

]
---
# Why LTE to choose controls
* Subjectivity
* Large nr of controls
* Results can be overturned by researcher who overfit the data


---

# Revisiting the abortion example

```{r}
library(gamlr)
## refactor state to have NA reference level
sna <- factor(s, levels=c(NA,levels(s)), exclude=NULL)
x <- sparse.model.matrix( ~ t + phone*sna + .^2, data=controls)[,-1]
dim(x)
## naive lasso regression
naive <- cv.gamlr(cbind(d,x),y); head(coef(naive)) 
coef(naive)["d",] 
```

---

```{r}
treat <- cv.gamlr(x,d, lmr=1e-3); head(summary(treat)) #we are modeling d as a function of x (treat is d)
predtreat <- predict(treat, x, select="min"); head(predtreat) #pred value from d
dhat <- drop(predtreat); length(dhat)
```
we would pick the smallest lambda and work out with lambda in the cv algorithm
We are computing out of sample r^2 for lambda and chose the beta that gives the smalles oos r^2
---

```{r}
par(mai=c(.9,.9,.1,.1))
plot(dhat,d,bty="n",pch=21,bg=8, cex=.8, yaxt="n")
axis(2, at=c(0,1,2,3)) 
## little to resemble an experiment here...
```

---

```{r}
## IS R^2? (in sample) 
cor(drop(dhat),d)^2 #we got an correlation of almost 1 so what is the added value? not so impressive --> why this comment in class if high rsq is usually good? it is not new information? problem was too straightforward? Maybe it is because of how we chose the model based to sample and then we will compare it to out of sample
## Note: IS R2 indicates how much independent signal you have for estimating 
coef(summary( glm( y ~ d + dhat) ))
# re-run lasso, with this (2nd column) included unpenalized (free=2). Second step of sam algo adding d, d hat and x. 
causal <- cv.gamlr(cbind(d,dhat,x),y,free=2,lmr=1e-3)
coef(causal, select="min")["d",] 
# AICc says abortion rate has no causal effect on crime.
# if we select minimum it says effect of d is 0 (min we learned last week)
```


---

Propensity models are a variant of the LTE framework for binary treatments. Role for logistic regression in these models. 

.pull-left[

\begin{align*}
y = \beta_1 d + \mathbf{x}'\beta + e \qquad \mathbb{E}[e|d,\mathbf{x}] = 0\\
d = \mathbf{x}'\tau  + v\qquad \mathbb{E}[v|\mathbf{x}] = 0 \\
\\
\\
\\
\\
\end{align*}

Lasso Linear Regression ALGORITHM:

  i. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[d|\mathbf{x}]=\mathbf{x}'\tau$. 
  
  Collect fitted values $\hat{d}= \mathbf{x}'\hat{\tau}$.
  
  ii. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[y|\mathbf{x},d] = \phi\hat{d} + \beta_1d + \mathbf{x}'\beta$
  
  Do not penalize $\phi$. 

]

.pull-right[


\begin{align*}
y = \beta_1 d + \mathbf{x}'\beta + e \qquad \mathbb{E}[e|d,\mathbf{x}] = 0\\
d \sim Bernouilli(q(\mathbf{x})), q(\mathbf{x})= \frac{exp(\mathbf{x}'\tau)}{1 + exp(\mathbf{x}'\tau)}\\
\\
\end{align*}
$q(\mathbf{x})$ is called the propensity score. 

Lasso Logistic Regression ALGORITHM:

  i. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[d|\mathbf{x}] = q(\mathbf{x})= \frac{exp(\mathbf{x}'\tau)}{1 + exp(\mathbf{x}'\tau)}$. 
  
  Collect fitted values $\hat{q}(\mathbf{x})= \frac{exp(\mathbf{x}'\hat{\tau})}{1 + exp(\mathbf{x}'\hat{\tau})}$.
  
  ii. Use cross validation ( $CV$ ) or $AIC_c$ Lasso to estimate
  
  $\mathbb{E}[y|\mathbf{x},d] = \phi\hat{q}(\mathbf{x}) + \beta_1d + \mathbf{x}'\beta$
  
  Do not penalize $\phi$. 

]


---

# Uncertainty quantification with lasso regressions

Usual inferential procedures do not apply in case of lasso regressions. How do you quantify uncertainty in these settings? That is, how do you construct standard errors?  

One simple algorithm is called sample splitting: 

  * Split sample into 2 pieces 
  * Do model selection on one piece 
  * Estimate selected model on other piece. 
  * Do inference on this piece. 
  
Let's look at an example.

---

```{r}
library(gamlr)
data(hockey)
head(goal, n=2) #we want to predict prob that home team scores. differential is other team scored
player[1:2, 2:7] #players on ice. +1 is home players. 0 is off ice. makes soarse matrix with who scored.
team[1, 2:6] #Sparse Matrix with indicators for each team*season interaction: +1 for home team, -1 for away team. 
config[5:6, 2:7] #Special teams info. For example, S5v4 is a 5 on 4 powerplay, +1 if it is for the home-team and -1 for the away team.
```

---

# Sample splitting example

```{r}
x <- cbind(config,team,player) #config who is in ice
y <- goal$homegoal
fold <- sample.int(2,nrow(x),replace=TRUE) # we want interference, fold is sekecting rows of x at random and replace it 
head(fold) #shows wheree rows go this is different for me and the slides
nhlprereg <- gamlr(x[fold==1,], y[fold==1], #design subset with fold==1. this code chunk tells how to subset the data
	free=1:(ncol(config)+ncol(team)), 
	family="binomial", standardize=FALSE)
selected <- which(coef(nhlprereg)[-1,] != 0)
xnotzero <- as.data.frame(as.matrix(x[,selected]))
nhlmle <- glm( y ~ ., data=xnotzero, 
			subset=which(fold==2), family=binomial )
```

---
Can observe standard errors via

```{r}
summary(nhlmle)
```

---

Suppose objective is to predict the probability that a goal was from the home team (Edmonton in case of first obs).

```{r}
x[1,x[1,]!=0] #check first observation for players on the ice
fit <- predict(nhlmle, xnotzero[1,,drop=FALSE], type="response", se.fit=TRUE)$fit; fit #predicted prob that home team scores
se.fit <- predict(nhlmle, xnotzero[1,,drop=FALSE], type="response", se.fit=TRUE)$se.fit; se.fit #we get the se for prediction. we dont want SE of b hat but of y hat. In applied econometrics we care about good parameter (b) but here we care about good predictions (y)
CI = fit + c(-2,2)*se.fit
CI #90% confidence interval for probability that Edmonton scored the goal is 
```
Everytime you sample the coefficient changes (bc u bootstrap) so two sources of uncertainty on that way: different estimate and different sample which gives different se

---

# Additional comments on sample splitting

  * Sample splitting delivered confidence intervals for the predicted values: 
  
    $\hat{y} = \frac{exp(\mathbf{\tilde{x}}'\hat{\beta})}{1 + exp(\mathbf{x}'\hat{\beta})}$

  * Uncertainty quantification in second subsample ignores model selection that took place in first stage. 
    + Typical argue that this okay because model selection is "nuisance". d hat is nuisance
      - Main interest is in one parameter. 
      - Other parameters are something we have to deal with. 
      - Not a big deal to throw away "unimportant" controls 
    + Makes sample splitting very useful in high dimensional contexts 
  
  * Sample splitting with high dimensional controls can be done in a more general way. The next algorithm shows you how. 
    
---

# References

```{r, 'refs1', results='asis', echo=FALSE, eval=TRUE}
PrintBibliography(bib, start = 1, end = 6)
```
